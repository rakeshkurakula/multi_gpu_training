{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch go-to choice for researchers and practitioners for building deep learning models.\n",
    "- It has some inherient problems like:\n",
    "    1. Managing training loops\n",
    "    2. Logging\n",
    "    3. Handling Distributed training\n",
    "    4. Debugging in a distributed setting\n",
    "    5. Mixed precision training\n",
    "    6. Running models on TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.3688920587301254, Accuracy: 0.104\n",
      "Epoch 2, Loss: 2.3498924151062965, Accuracy: 0.097\n",
      "Epoch 3, Loss: 2.335986942052841, Accuracy: 0.099\n",
      "Epoch 4, Loss: 2.3231282085180283, Accuracy: 0.101\n",
      "Epoch 5, Loss: 2.3174316734075546, Accuracy: 0.099\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset\n",
    "train_dataset = torch.utils.data.TensorDataset(torch.randn(1000, 2), torch.randint(0, 10, (1000,)))\n",
    "test_dataset = torch.utils.data.TensorDataset(torch.randn(1000, 2), torch.randint(0, 10, (1000,)))\n",
    "val_dataset = torch.utils.data.TensorDataset(torch.randn(1000, 2), torch.randint(0, 10, (1000,)))\n",
    "\n",
    "# Defining the model class inherited from PyTorch's nn.Module class\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the evaluate function with proper batching\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# we have declared the model without backward pass explicitly\n",
    "epochs = 5\n",
    "model = MyModel()\n",
    "# Defining the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    accuracy = evaluate(model)\n",
    "    # print the loss and accuracy\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As discussed above there are problems with PyTorch\n",
    "\n",
    "# PyTorch Lighting \n",
    "- Solves the above discussed challenges\n",
    "\n",
    "- Managing training loops: PyTorch Lightning simplifies this process by providing a high-level abstraction for defining the training loop, reducing the amount of boilerplate code required.\n",
    "- Logging: PyTorch Lightning integrates with popular logging frameworks like TensorBoard and Comet, making it easier to log training metrics and visualize them in real-time.\n",
    "Handling distributed training: PyTorch Lightning simplifies distributed training by providing a unified interface. This abstracts away the complexity of the underlying implementation.\n",
    "- Debugging in a distributed setting: PyTorch Lightning provides tools and utilities to facilitate debugging in a distributed setting, making it easier to identify and resolve issues.\n",
    "- Mixed-precision training: PyTorch Lightning simplifies mixed-precision training by providing utilities to automatically handle the precision of operations based on user-defined settings.\n",
    "- Running models on TPUs: PyTorch Lightning supports running models on TPUs, abstracting away the complexity of the underlying TPU architecture and allowing users to focus on their model implementation.\n",
    "\n",
    "# PyTorch to PyTorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definint the PyTorch model\n",
    "\n",
    "# importing the required packages and libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the dataset\n",
    "\n",
    "# 1) Data transformer\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# 2) Create Train dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, \n",
    "                                      download=True, transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 3) Create Test dataset\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, \n",
    "                                     download=True, transform=transform)\n",
    "\n",
    "# 4) Create DataLoader\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that the data is loaded, we can define the model\n",
    "\n",
    "class PyTorchNet(nn.Module):\n",
    "    \n",
    "    # Defining the architecture of the model\n",
    "    def __init__(self):\n",
    "        super(PyTorchNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 128)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "\n",
    "    # Defining the forward pass\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "# Initialzie the model and define the loss function and optimizer\n",
    "model = PyTorchNet()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define a evaluation methods\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\n",
      "          Loss: 0.29247100143306165,\n",
      "          Accuracy: 0.9276\n",
      "          \n",
      "Epoch 2,\n",
      "          Loss: 0.14089426057470808,\n",
      "          Accuracy: 0.9632\n",
      "          \n",
      "Epoch 3,\n",
      "          Loss: 0.107698312252889,\n",
      "          Accuracy: 0.9661\n",
      "          \n",
      "Epoch 4,\n",
      "          Loss: 0.09020289394017587,\n",
      "          Accuracy: 0.9727\n",
      "          \n",
      "Epoch 5,\n",
      "          Loss: 0.0748992892322756,\n",
      "          Accuracy: 0.9729\n",
      "          \n"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for data in trainloader:\n",
    "\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    accuracy = evaluate(model)\n",
    "\n",
    "    print(f\"\"\"Epoch {epoch + 1},\n",
    "          Loss: {running_loss / len(trainloader)},\n",
    "          Accuracy: {accuracy}\n",
    "          \"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Lightning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "class PyTorchLightningNet(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PyTorchLightningNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 128)\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = PyTorchLightningNet()\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define a evaluation methods\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\n",
      "          Loss: 0.30681939999352514,\n",
      "          Accuracy: 0.9312\n",
      "          \n",
      "Epoch 2,\n",
      "          Loss: 0.1400705497435876,\n",
      "          Accuracy: 0.9451\n",
      "          \n",
      "Epoch 3,\n",
      "          Loss: 0.10690825645710582,\n",
      "          Accuracy: 0.9624\n",
      "          \n",
      "Epoch 4,\n",
      "          Loss: 0.09134310164175101,\n",
      "          Accuracy: 0.9683\n",
      "          \n",
      "Epoch 5,\n",
      "          Loss: 0.07344570693729727,\n",
      "          Accuracy: 0.9702\n",
      "          \n"
     ]
    }
   ],
   "source": [
    "## Train the model\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for data in trainloader:\n",
    "\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    accuracy = evaluate(model)\n",
    "\n",
    "    print(f\"\"\"Epoch {epoch + 1},\n",
    "          Loss: {running_loss / len(trainloader)},\n",
    "          Accuracy: {accuracy}\n",
    "          \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like there is more variation in the accuracy\n",
    "\n",
    "To do this we can add things to out Lightining model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakeshk94/Desktop/multi_gpu_training/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:513: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/rakeshk94/Desktop/multi_gpu_training/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name      | Type                | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | model     | Sequential          | 1.2 M  | train\n",
      "1 | criterion | CrossEntropyLoss    | 0      | train\n",
      "2 | accuracy  | MulticlassAccuracy  | 0      | train\n",
      "3 | precision | MulticlassPrecision | 0      | train\n",
      "----------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.895     Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakeshk94/Desktop/multi_gpu_training/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/rakeshk94/Desktop/multi_gpu_training/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training!!\n",
      "Epoch 3: 100%|██████████| 938/938 [00:55<00:00, 17.03it/s, v_num=17, train_loss_step=0.0485, train_acc_step=1.000, train_precision_step=1.000, val_loss=0.114, val_acc=0.966, train_loss_epoch=0.0923, train_acc_epoch=0.972, train_precision_epoch=0.972]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 938/938 [00:55<00:00, 17.02it/s, v_num=17, train_loss_step=0.0485, train_acc_step=1.000, train_precision_step=1.000, val_loss=0.114, val_acc=0.966, train_loss_epoch=0.0923, train_acc_epoch=0.972, train_precision_epoch=0.972]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rakeshk94/Desktop/multi_gpu_training/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 181.52it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9656000137329102\n",
      "        test_loss           0.11355363577604294\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.11355363577604294, 'test_acc': 0.9656000137329102}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import os\n",
    "\n",
    "\n",
    "# Empty MPS cache before training (helps free up memory)\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "# Data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class TrainingProgressCallback(pl.Callback):\n",
    "\n",
    "    def on_train_start(self, trainer, pl_module):\n",
    "        print(\"Starting training!!\")\n",
    "\n",
    "# Define Lightning model\n",
    "class MNISTLightningModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),  # First FC layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),  # Second FC layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 256),  # Third FC layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),  # Fourth FC layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)  # Output layer\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Define torchmetrics for tracking\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.precision = torchmetrics.Precision(task=\"multiclass\", num_classes=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten input\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        # Compute metrics (convert logits to class predictions)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "\n",
    "        acc = self.accuracy(preds, labels)\n",
    "        prec = self.precision(preds, labels)\n",
    "\n",
    "        # Log metrics separately\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train_precision', prec, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        acc = self.accuracy(preds, labels)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        acc = self.accuracy(preds, labels)\n",
    "\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return test_loader\n",
    "\n",
    "\n",
    "# Initialize model and trainer\n",
    "model = MNISTLightningModel()\n",
    "trainer = pl.Trainer(max_epochs=4,\n",
    "                     accelerator= \"cpu\",\n",
    "                     precision=16,\n",
    "                     log_every_n_steps=10,\n",
    "                     callbacks=[TrainingProgressCallback()])\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Test the model\n",
    "trainer.test(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.predict(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenient features\n",
    "1. Callbacks\n",
    "2. profiling\n",
    "3. Mixed precision training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9(torch cuda)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
